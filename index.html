<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="/css/custom.css"> <link rel=icon  href="/assets/favicon.png"> <title>JuliaDiff</title> <div id=layout > <div id=menu > <ul> </ul> </div> <div id=main > <div class=franklin-content ><div class=topmatter ><h1 id=juliadiff ><a href="#juliadiff">JuliaDiff</a></h1> <p>Differentiation tools in <a href="https://julialang.org">Julia</a>. <a href="https://github.com/JuliaDiff/">JuliaDiff on GitHub</a></p></div> <h2 id=stop_approximating_derivatives ><a href="#stop_approximating_derivatives">Stop approximating derivatives&#33;</a></h2> <p>Derivatives are required at the core of many numerical algorithms. Unfortunately, they are usually computed <em>inefficiently</em> and <em>approximately</em> by some variant of the finite difference approach</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msup><mi>f</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msup><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo>≈</mo><mfrac><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo stretchy=false >)</mo><mo>−</mo><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><mi>h</mi></mfrac><mo separator=true >,</mo><mi>h</mi><mtext> small </mtext><mi mathvariant=normal >.</mi></mrow><annotation encoding="application/x-tex"> f&#x27;(x) \approx \frac{f(x+h) - f(x)}{h}, h \text{ small }. </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.051892em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >≈</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.113em;vertical-align:-0.686em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.427em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal">h</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">h</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">h</span><span class="mord text"><span class=mord > small </span></span><span class=mord >.</span></span></span></span></span> <p>This method is <em>inefficient</em> because it requires <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >Ω</mi><mo stretchy=false >(</mo><mi>n</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \Omega(n) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >Ω</span><span class=mopen >(</span><span class="mord mathnormal">n</span><span class=mclose >)</span></span></span></span> evaluations of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi mathvariant=double-struck >R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant=double-struck >R</mi></mrow><annotation encoding="application/x-tex"> f : \mathbb{R}^n \to \mathbb{R} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >:</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.68889em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord mathbb">R</span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.68889em;vertical-align:0em;"></span><span class=mord ><span class="mord mathbb">R</span></span></span></span></span> to compute the gradient <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >∇</mi><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo>=</mo><mrow><mo fence=true >(</mo><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>f</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo separator=true >,</mo><mo>⋯</mo><mtext> </mtext><mo separator=true >,</mo><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>f</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo fence=true >)</mo></mrow></mrow><annotation encoding="application/x-tex"> \nabla f(x) = \left( \frac{\partial f}{\partial x_1}(x), \cdots, \frac{\partial f}{\partial x_n}(x)\right) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.80002em;vertical-align:-0.65002em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.9322159999999999em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=minner >⋯</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.9322159999999999em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span>, for example. It is <em>approximate</em> because we have to choose some finite, small value of the step length <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex"> h </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span>, balancing floating-point precision with mathematical approximation error.</p> <h4 id=what_can_we_do_instead ><a href="#what_can_we_do_instead">What can we do instead?</a></h4> <p>One option is to explicitly write down a function which computes the exact derivatives by using the rules that we know from calculus. However, this quickly becomes an error-prone and tedious exercise. <strong>There is another way&#33;</strong> The field of <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> provides methods for automatically computing <em>exact</em> derivatives &#40;up to floating-point error&#41; given only the function <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> itself. Some methods use many fewer evaluations of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> than would be required when using finite differences. In the best case, <strong>the exact gradient of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> can be evaluated for the cost of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy=false >(</mo><mn>1</mn><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> O(1) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=mopen >(</span><span class=mord >1</span><span class=mclose >)</span></span></span></span> evaluations of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> itself</strong>. The caveat is that <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> cannot be considered a black box; instead, we require either access to the source code of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex"> f </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> or a way to plug in a special type of number using operator overloading.</p> <h2 id=what_is_juliadiff ><a href="#what_is_juliadiff">What is JuliaDiff?</a></h2> <p>JuliaDiff is an informal organization which aims to unify and document packages written in <a href="https://julialang.org">Julia</a> for evaluating derivatives. The technical features of Julia, namely, multiple dispatch, source code via reflection, JIT compilation, and first-class access to expression parsing make implementing and using techniques from automatic differentiation easier than ever before &#40;in our biased opinion&#41;.</p> <h2 id=the_big_list ><a href="#the_big_list">The Big List</a></h2> <p>This is a big list of Julia Automatic Differentiation &#40;AD&#41; packages and related tooling. As you can see there is a lot going on here. As with any such big lists it rapidly becomes out-dated. When you notice something that is out of date, or just plain wrong, please <a href="https://github.com/JuliaDiff/juliadiff.github.io">submit a PR</a>.</p> <p>This list aims to be comprehensive in coverage. By necessity, this means it is not comprehensive in detail. It is worth investigating each package yourself to really understand its ins and outs, and pros and cons of its competitors.</p> <h3 id=reverse-mode ><a href="#reverse-mode">Reverse-mode</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a>: Operator overloading reverse-mode AD. Very well-established.</p> <li><p><a href="https://github.com/invenia/Nabla.jl/">Nabla.jl</a>: Operator overloading reverse-mode AD. Used in &#40;its maintainer&#41; Invenia&#39;s systems. </p> <li><p><a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a>: Operator overloading reverse-mode AD. Most well-known for having been the AD used in earlier versions of the machine learning package <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>. No longer used by Flux.jl, but still used in several places in the Julia ecosystem.</p> <li><p><a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a>: Operator overloading reverse-mode AD. Originally a port of the <a href="https://github.com/HIPS/autograd">Python Autograd package</a>. Primarily used in <a href="https://github.com/denizyuret/Knet.jl/">Knet.jl</a>.</p> <li><p><a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>: IR-level source to source reverse-mode AD. Very widely used. Particularly notable for being the AD used by <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>. Also features a secret experimental source to source forward-mode AD.</p> <li><p><a href="https://github.com/dfdx/Yota.jl">Yota.jl</a>: IR-level source to source reverse-mode AD.</p> <li><p><a href="https://github.com/dfdx/XGrad.jl">XGrad.jl</a>: AST-level source to source reverse-mode AD. Not currently in active development.</p> <li><p><a href="https://github.com/dpsanders/ReversePropagation.jl">ReversePropagation.jl</a>: Scalar, tracing-based source to source reverse-mode AD.</p> <li><p><a href="https://github.com/wsmoses/Enzyme.jl">Enzyme.jl</a>: Scalar, LLVM source to source reverse-mode AD. Experimental.</p> <li><p><a href="https://github.com/JuliaDiff/Diffractor.jl">Diffractor.jl</a>: Next-gen IR-level source to source reverse-mode &#40;and forward-mode&#41; AD. In development.</p> </ul> <h3 id=forward-mode ><a href="#forward-mode">Forward-mode</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a>: Scalar, operator overloading forward-mode AD. Very stable. Very well-established.</p> <li><p><a href="https://github.com/YingboMa//ForwardDiff2.jl">ForwardDiff2</a>: Experimental, non-scalar hybrid operator-overloading/source-to-source forward-mode AD. Not currently in development.</p> </ul> <h3 id=symbolic ><a href="#symbolic">Symbolic:</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiffEq/ModelingToolkit.jl">ModelingToolKit.jl</a>: A pure Julia <a href="https://en.wikipedia.org/wiki/Computer_algebra_system">computer algebra system</a>. While its docs focus on some particular domain use-case it is a fully general purpose system.</p> </ul> <h3 id=exotic ><a href="#exotic">Exotic</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/TaylorSeries.jl">TaylorSeries.jl</a>: Computes polynomial expansions; which is the generalization of forward-mode AD to nth-order derivatives.</p> <li><p><a href="https://github.com/GiggleLiu/NiLang.jl">NiLang.jl</a>: <a href="https://en.wikipedia.org/wiki/Reversible_computing">Reversible computing</a> <a href="https://en.wikipedia.org/wiki/Domain-specific_language">DSL</a>, where everything is differentiable by reversing.</p> </ul> <h3 id=finite_differencing ><a href="#finite_differencing">Finite Differencing</a></h3> <p>Yes, we said at the start to stop approximating derivatives, but these packages are faster and more accurate than you would expect finite differencing to ever achieve. If you really need finite differencing, use these packages rather than implementing your own.</p> <ul> <li><p><a href="https://github.com/JuliaDiff/FiniteDifferences.jl">FiniteDifferences.jl</a>: High-accuracy finite differencing with support for almost any type &#40;not just arrays and numbers&#41;.</p> <li><p><a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl</a>: High-accuracy finite differencing with support for efficient calculation of sparse Jacobians via coloring vectors.</p> <li><p><a href="https://github.com/JuliaMath/Calculus.jl">Calculus.jl</a>: Largely deprecated, legacy package. New users should look to FiniteDifferences.jl and FiniteDiff.jl instead.</p> </ul> <h3 id=rulesets ><a href="#rulesets">Rulesets</a></h3> <p>Packages providing collections of derivatives of functions which can be used in AD packages.</p> <ul> <li><p><a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/">ChainRules</a>: Extensible, AD-independent rules.</p> <ul> <li><p><a href="https://github.com/JuliaDiff/ChainRulesCore.jl">ChainRulesCore.jl</a>: Core API for user to extend to add rules to their package.</p> <li><p><a href="https://github.com/JuliaDiff/ChainRules.jl/">ChainRules.jl</a>: Rules for Julia Base and standard libraries.</p> <li><p><a href="https://github.com/JuliaDiff/ChainRulesTestUtils.jl/">ChainRulesTestUtils.jl</a>: Tools for testing rules defined with ChainRulesCore.jl.</p> </ul> <li><p><a href="https://github.com/JuliaDiff/DiffRules.jl">DiffRules.jl</a>: An earlier set of AD-independent rules, for scalar functions. Used as the primary source for ForwardDiff.jl, and in part by other packages.</p> <li><p><a href="https://github.com/FluxML/ZygoteRules.jl">ZygoteRules.jl</a>: Lightweight package for defining rules for Zygote.jl. Largely deprecated in favour of the AD-independent ChainRulesCore.jl.</p> </ul> <h3 id=sparsity ><a href="#sparsity">Sparsity</a></h3> <ul> <li><p><a href="https://github.com/SciML/SparsityDetection.jl">SparsityDetection.jl</a>: Automatic Jacobian and Hessian sparsity pattern detection.</p> <li><p><a href="https://github.com/JuliaDiff/SparseDiffTools.jl">SparseDiffTools.jl</a>: Exploiting sparsity to speed up FiniteDiff.jl and ForwardDiff.jl, as well as other algorithms.</p> </ul> <h2 id=links_for_discussion_and_more_information ><a href="#links_for_discussion_and_more_information">Links for discussion and more information</a></h2> <p>Discussions on JuliaDiff and its uses may be directed to the <a href="https://discourse.julialang.org/">Julia Discourse forum</a> The <a href="http://www.autodiff.org/">autodiff.org</a> site serves as a portal for the academic community, though it is often out-of-date. The ChainRules project maintains a <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/FAQ.html#Where-can-I-learn-more-about-AD-?">list of recommend reading/watching</a> for those after more information. Finally, automatic differentiation techniques have been implemented in a variety of languages. If you would prefer not to use Julia, see the <a href="http://en.wikipedia.org/wiki/Automatic_differentiation">wikipedia page</a> for a comprehensive list of available packages.</p> <div class=page-foot > <div class=copyright > &copy; Lyndon White, Miles Lubin. Last modified: July 26, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>