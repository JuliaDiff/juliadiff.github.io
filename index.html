<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="/css/custom.css"> <link rel=icon  href="/assets/favicon.png"> <title>JuliaDiff</title> <div id=layout > <div id=menu > <ul> </ul> </div> <div id=main > <div class=franklin-content ><div class=topmatter ><h1 id=juliadiff ><a href="#juliadiff" class=header-anchor >JuliaDiff</a></h1> <p>Differentiation tools in Julia</p></div> <h2 id=computing_derivatives ><a href="#computing_derivatives" class=header-anchor >Computing derivatives</a></h2> <p>Derivatives are required by many numerical algorithms, even for functions <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> given as chunks of computer code rather than simple mathematical expressions. There are various ways to compute the gradient, Jacobian or Hessian of such functions without tedious manual labor:</p> <ul> <li><p>Symbolic differentiation, which uses <a href="https://en.wikipedia.org/wiki/Computer_algebra">computer algebra</a> to work out explicit formulas</p> <li><p><a href="https://en.wikipedia.org/wiki/Numerical_differentiation">Numerical differentiation</a>, which relies on variants of the finite difference approximation <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msup><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo>≈</mo><mfrac><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo>+</mo><mi>ε</mi><mo stretchy=false >)</mo><mo>−</mo><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><mi>ε</mi></mfrac></mrow><annotation encoding="application/x-tex">f&#x27;(x) \approx \frac{f(x+\varepsilon) - f(x)}{\varepsilon}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.001892em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >≈</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.355em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ε</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ε</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p> <li><p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a> &#40;AD&#41;, which reinterprets the code of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> either in &quot;forward&quot; or &quot;reverse&quot; mode</p> </ul> <p>In machine learning, automatic differentiation is probably the most widely used paradigm, especially in reverse mode. However, each method has its own upsides and tradeoffs, which are detailed in the following papers, along with implementation techniques like &quot;operator overloading&quot; and &quot;source transformation&quot;:</p> <blockquote> <p><a href="https://jmlr.org/papers/v18/17-468.html"><em>Automatic differentiation in machine learning: a survey</em></a>, Baydin et al. &#40;2018&#41;</p> </blockquote> <blockquote> <p><a href="https://arxiv.org/abs/1811.05031"><em>A review of automatic differentiation and its efficient implementation</em></a>, Margossian &#40;2019&#41;</p> </blockquote> <h2 id=what_is_juliadiff ><a href="#what_is_juliadiff" class=header-anchor >What is JuliaDiff?</a></h2> <p>JuliaDiff is an informal <a href="https://github.com/JuliaDiff/">GitHub organization</a> which aims to unify and document packages written in <a href="https://julialang.org">Julia</a> for evaluating derivatives. The technical features of Julia<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup> make implementing and using differentiation techniques easier than ever before &#40;in our biased opinion&#41;.</p> <p>Discussions on JuliaDiff and its uses may be directed to the <a href="https://discourse.julialang.org/">Julia Discourse forum</a>. The ChainRules project maintains a <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/FAQ.html#Where-can-I-learn-more-about-AD-?">list of recommended reading</a> for those after more information. The <a href="http://www.autodiff.org/">autodiff.org</a> site serves as a portal for the academic community, though it is often out of date.</p> <h2 id=the_big_list ><a href="#the_big_list" class=header-anchor >The Big List</a></h2> <p>What follows is a big list of Julia differentiation packages and related tooling, last updated in January 2024. If you notice something inaccurate or outdated, please <a href="https://github.com/JuliaDiff/juliadiff.github.io/issues">open an issue</a> to signal it. The packages marked as inactive are those which have had no release in 2023.</p> <p>The list aims to be comprehensive in coverage. By necessity, this means it is not comprehensive in detail. It is worth investigating each package yourself to really understand its ins and outs, and the pros and cons of its competitors.</p> <h3 id=reverse_mode_automatic_differentiation ><a href="#reverse_mode_automatic_differentiation" class=header-anchor >Reverse mode automatic differentiation</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/ReverseDiff.jl">JuliaDiff/ReverseDiff.jl</a>: Operator overloading AD backend</p> <li><p><a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a>: Source transformation AD backend</p> <li><p><a href="https://github.com/EnzymeAD/Enzyme.jl">EnzymeAD/Enzyme.jl</a>: LLVM-level source transformation AD backend</p> <li><p><a href="https://github.com/FluxML/Tracker.jl">FluxML/Tracker.jl</a>: Operator overloading AD backend</p> <li><p><a href="https://github.com/compintell/Tapir.jl">compintell/Tapir.jl</a>: Source transformation AD backend &#40;experimental&#41;</p> <li><p><a href="https://github.com/dfdx/Yota.jl">dfdx/Yota.jl</a>: Source transformation AD backend</p> </ul> <h3 id=forward_mode_automatic_differentiation ><a href="#forward_mode_automatic_differentiation" class=header-anchor >Forward mode automatic differentiation</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/ForwardDiff.jl">JuliaDiff/ForwardDiff.jl</a>: Operator overloading AD backend</p> <li><p><a href="https://github.com/JuliaDiff/PolyesterForwardDiff.jl">JuliaDiff/PolyesterForwardDiff.jl</a>: Multithreaded version of ForwardDiff.jl</p> <li><p><a href="https://github.com/EnzymeAD/Enzyme.jl">EnzymeAD/Enzyme.jl</a>: LLVM-level source transformation AD backend</p> <li><p><a href="https://github.com/JuliaDiff/Diffractor.jl">JuliaDiff/Diffractor.jl</a>: Source transformation AD backend &#40;experimental&#41;</p> </ul> <h3 id=symbolic_differentiation ><a href="#symbolic_differentiation" class=header-anchor >Symbolic differentiation</a></h3> <ul> <li><p><a href="https://github.com/JuliaSymbolics/Symbolics.jl">JuliaSymbolics/Symbolics.jl</a>: Pure Julia computer algebra system with support for fast and sparse analytical derivatives</p> <li><p><a href="https://github.com/brianguenter/FastDifferentiation.jl">brianguenter/FastDifferentiation.jl</a>: Generate efficient executables for symbolic derivatives</p> </ul> <h3 id=numeric_differentiation ><a href="#numeric_differentiation" class=header-anchor >Numeric differentiation</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/FiniteDifferences.jl">JuliaDiff/FiniteDifferences.jl</a>: Finite differences with support for arbitrary types and higher order schemes</p> <li><p><a href="https://github.com/JuliaDiff/FiniteDiff.jl">JuliaDiff/FiniteDiff.jl</a>: Finite differences with support for caching and sparsity</p> </ul> <h3 id=higher_order ><a href="#higher_order" class=header-anchor >Higher order</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/TaylorSeries.jl">JuliaDiff/TaylorSeries.jl</a>: Taylor polynomial expansions in one or more variables</p> <li><p><a href="https://github.com/JuliaDiff/TaylorDiff.jl">JuliaDiff/TaylorDiff.jl</a>: Higher order directional derivatives &#40;experimental&#41;</p> <li><p><a href="https://github.com/JuliaDiff/Diffractor.jl">JuliaDiff/Diffractor.jl</a>: Source transformation AD backend &#40;experimental&#41;</p> </ul> <h3 id=interfaces ><a href="#interfaces" class=header-anchor >Interfaces</a></h3> <ul> <li><p><a href="https://github.com/gdalle/DifferentiationInterface.jl">gdalle/DifferentiationInterface.jl</a>: Generic interface for first- and second-order differentiation with any AD backend on 1-argument functions &#40;<code>f&#40;x&#41; &#61; y</code> or <code>f&#33;&#40;y, x&#41;</code>&#41;.</p> <li><p><a href="https://github.com/JuliaDiff/AbstractDifferentiation.jl">JuliaDiff/AbstractDifferentiation.jl</a>: Generic interface for first- and second-order differentiation with a subset of AD backends on functions with more than one argument &#40;will soon wrap DifferentiationInterface.jl&#41;.</p> </ul> <h3 id=rulesets ><a href="#rulesets" class=header-anchor >Rulesets</a></h3> <p>These packages define derivatives for basic functions, and enable users to do the same:</p> <ul> <li><p><a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/">JuliaDiff/ChainRules</a>: Ecosystem for backend-agnostic forward and reverse rules.</p> <ul> <li><p><a href="https://github.com/JuliaDiff/ChainRulesCore.jl">JuliaDiff/ChainRulesCore.jl</a>: Core API for users to add rules to their package.</p> <li><p><a href="https://github.com/JuliaDiff/ChainRules.jl/">JuliaDiff/ChainRules.jl</a>: Rules for Julia Base and standard libraries.</p> <li><p><a href="https://github.com/JuliaDiff/ChainRulesTestUtils.jl/">JuliaDiff/ChainRulesTestUtils.jl</a>: Tools for testing rules defined with ChainRulesCore.jl.</p> </ul> <li><p><a href="https://github.com/ThummeTo/ForwardDiffChainRules.jl">ThummeTo/ForwardDiffChainRules.jl</a>: Translate rules from ChainRulesCore.jl to make them compatible with ForwardDiff.jl</p> <li><p><a href="https://github.com/JuliaDiff/DiffRules.jl">JuliaDiff/DiffRules.jl</a>: Scalar rules used by e.g. ForwardDiff.jl, ReverseDiff.jl, Tracker.jl, and Symbolics.jl</p> <li><p><a href="https://enzymead.github.io/Enzyme.jl/stable/generated/custom_rule/">EnzymeAD/EnzymeRules.jl</a>: Rule definition API for Enzyme.jl</p> <li><p><a href="https://github.com/FluxML/ZygoteRules.jl">FluxML/ZygoteRules.jl</a>: Some rules used by Zygote.jl &#40;mostly deprecated in favor of ChainRules.jl&#41;.</p> </ul> <h3 id=sparsity ><a href="#sparsity" class=header-anchor >Sparsity</a></h3> <ul> <li><p><a href="https://github.com/JuliaDiff/SparseDiffTools.jl">JuliaDiff/SparseDiffTools.jl</a>: Exploit sparsity to speed up FiniteDiff.jl and ForwardDiff.jl, as well as other algorithms.</p> <li><p><a href="https://github.com/adrhill/SparseConnectivityTracer.jl">adrhill/SparseConnectivityTracer.jl</a>: Sparsity pattern detection for Jacobians and Hessians.</p> <li><p><a href="https://github.com/gdalle/SparseMatrixColorings.jl">gdalle/SparseMatrixColorings.jl</a>: Efficient coloring and and decompression algorithms for sparse Jacobians and Hessians.</p> </ul> <h3 id=differentiating_through_more_stuff ><a href="#differentiating_through_more_stuff" class=header-anchor >Differentiating through more stuff</a></h3> <p>Some complex algorithms are not natively differentiable, which is why derivatives have been implemented in the following packages:</p> <ul> <li><p><a href="https://github.com/SciML">SciML</a>: For a lot of different domains of scientific machine learning: differential equations, linear and nonlinear systems, optimization problems, etc.</p> <li><p><a href="https://github.com/gdalle/ImplicitDifferentiation.jl">gdalle/ImplicitDifferentiation.jl</a>: For generic algorithms specified by output conditions, thanks to the implicit function theorem</p> <li><p><a href="https://github.com/jump-dev/DiffOpt.jl">jump-dev/DiffOpt.jl</a>: For convex optimization problems</p> <li><p><a href="https://github.com/axelparmentier/InferOpt.jl">axelparmentier/InferOpt.jl</a>: For combinatorial optimization problems</p> <li><p><a href="https://github.com/gaurav-arya/StochasticAD.jl">gaurav-arya/StochasticAD.jl</a>: Differentiation of functions with stochastic behavior &#40;experimental&#41;</p> </ul> <h3 id=inactive_packages ><a href="#inactive_packages" class=header-anchor >Inactive packages</a></h3> <ul> <li><p><a href="https://github.com/denizyuret/AutoGrad.jl">denizyuret/AutoGrad.jl</a></p> <li><p><a href="https://github.com/dfdx/XGrad.jl">dfdx/XGrad.jl</a></p> <li><p><a href="https://github.com/dpsanders/ReversePropagation.jl">dpsanders/ReversePropagation.jl</a></p> <li><p><a href="https://github.com/GiggleLiu/NiLang.jl">GiggleLiu/NiLang.jl</a></p> <li><p><a href="https://github.com/invenia/Nabla.jl/">invenia/Nabla.jl</a></p> <li><p><a href="https://github.com/JuliaDiff/DualNumbers.jl">JuliaDiff/DualNumbers.jl</a></p> <li><p><a href="https://github.com/JuliaMath/Calculus.jl">JuliaMath/Calculus.jl</a></p> <li><p><a href="https://github.com/rejuvyesh/PyCallChainRules.jl">rejuvyesh/PyCallChainRules.jl</a></p> <li><p><a href="https://github.com/SciML/SparsityDetection.jl">SciML/SparsityDetection.jl</a></p> <li><p><a href="https://github.com/YingboMa//ForwardDiff2.jl">YingboMa/ForwardDiff2</a></p> </ul> <table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >namely, multiple dispatch, source code via reflection, just-in-time compilation, and first-class access to expression parsing </table> <div class=page-foot > <div class=copyright > &copy; Frames White, Miles Lubin, Guillaume Dalle. Last modified: May 30, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div>